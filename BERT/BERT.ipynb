{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "xMxBiY-KOblT",
      "metadata": {
        "id": "xMxBiY-KOblT"
      },
      "source": [
        "# BERT Implementation for Text Classification in different Noise Levels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WVTjvGkYOh05",
      "metadata": {
        "id": "WVTjvGkYOh05"
      },
      "source": [
        "### Install dependecies + connect drive for dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sdJu6Cqs1fyH",
      "metadata": {
        "id": "sdJu6Cqs1fyH"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "r81U1Amr9_Fw",
      "metadata": {
        "id": "r81U1Amr9_Fw"
      },
      "outputs": [],
      "source": [
        "#!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c2e1ea9-ca27-429d-8322-6dbbbe27bee6",
      "metadata": {
        "id": "0c2e1ea9-ca27-429d-8322-6dbbbe27bee6"
      },
      "outputs": [],
      "source": [
        "#standard imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#Pytorch imports\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "\n",
        "#bert imports\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertModel\n",
        "\n",
        "#for precision, recall, fscore\n",
        "from sklearn.metrics import precision_recall_fscore_support as prfs\n",
        "\n",
        "#for train loop\n",
        "from tqdm import tqdm\n",
        "\n",
        "#for noise injection (file needs to be uploaded)\n",
        "from utils import noisify\n",
        "\n",
        "#set param\n",
        "np.random.seed(112)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56757332-e757-4d0f-8394-32dc870ffeba",
      "metadata": {
        "id": "56757332-e757-4d0f-8394-32dc870ffeba"
      },
      "source": [
        "### Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tJtDatdL1nz3",
      "metadata": {
        "id": "tJtDatdL1nz3"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab_Notebooks/bbc-text.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bd90c80-5a10-4feb-9e68-30235d853aa0",
      "metadata": {
        "id": "0bd90c80-5a10-4feb-9e68-30235d853aa0"
      },
      "outputs": [],
      "source": [
        "#label mapping\n",
        "labels = {'business':0,\n",
        "          'entertainment':1,\n",
        "          'sport':2,\n",
        "          'tech':3,\n",
        "          'politics':4\n",
        "          }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdf8449e-4075-492f-9a39-c93504039a43",
      "metadata": {
        "id": "fdf8449e-4075-492f-9a39-c93504039a43"
      },
      "source": [
        "### Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ea739f5-0db5-4f2e-bec3-6a7de3d5b4ba",
      "metadata": {
        "id": "0ea739f5-0db5-4f2e-bec3-6a7de3d5b4ba"
      },
      "outputs": [],
      "source": [
        "#init bert tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d8e9124-1b3c-439a-b7c4-cc9a9f786391",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d8e9124-1b3c-439a-b7c4-cc9a9f786391",
        "outputId": "1e292c3e-83e9-4eb2-e8ab-42b3da6d00ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1780 222 223\n"
          ]
        }
      ],
      "source": [
        "#split dataset into 80/10/10\n",
        "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42), [int(.8*len(df)), int(.9*len(df))])\n",
        "\n",
        "print(len(df_train),len(df_val), len(df_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Xw_1L5L86YA-",
      "metadata": {
        "id": "Xw_1L5L86YA-"
      },
      "source": [
        "### Bert Dataset and Bert Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yp2Roc5w1sRj",
      "metadata": {
        "id": "yp2Roc5w1sRj"
      },
      "outputs": [],
      "source": [
        "#custom Pytorch Dataset\n",
        "class BertDataset(Dataset):\n",
        "\n",
        "    def __init__(self, df, noise_rate=0.2, noise_type='clean', random_state=0):\n",
        "\n",
        "        ##Data Conversion\n",
        "\n",
        "        #label texts to numbers = Y\n",
        "        self.labels = [labels[label] for label in df['category']]\n",
        "\n",
        "        #tokenize texts = X\n",
        "        self.texts = [tokenizer(text, \n",
        "                               padding='max_length', max_length = 512, truncation=True,\n",
        "                                return_tensors=\"pt\") for text in df['text']]\n",
        "\n",
        "        ##\n",
        "\n",
        "\n",
        "\n",
        "        ##Noisify\n",
        "\n",
        "        #set params for noisify function\n",
        "        self.noise_type=noise_type\n",
        "        self.noise_rate=noise_rate\n",
        "        self.dataset='bbc'\n",
        "        \n",
        "        #inject noise\n",
        "        if noise_type != 'clean':\n",
        "            #convert Y to numpy array\n",
        "            self.labels=np.asarray([[self.labels[i]] for i in range(len(self.labels))])\n",
        "\n",
        "            #convert Y to noisy Y (is actually a different variable, so you still have access to the clean and the noisy labels simultaneously)\n",
        "            self.noisy_labels, self.actual_noise_rate = noisify(dataset=self.dataset, train_labels=self.labels, noise_type=self.noise_type, noise_rate=self.noise_rate, random_state=random_state)\n",
        "            self.noisy_labels=[i[0] for i in self.noisy_labels]\n",
        "\n",
        "        ##\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __get_batch_labels__(self, idx):\n",
        "\n",
        "        # Fetch a batch of labels = Y\n",
        "        if self.noise_type != 'clean':\n",
        "            return np.array(self.noisy_labels[idx])\n",
        "        return np.array(self.labels[idx])\n",
        "\n",
        "    def __get_batch_texts__(self, idx):\n",
        "\n",
        "        # Fetch a batch of inputs = X\n",
        "        return self.texts[idx]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        #Fetch a batch of data pairs (X, Y)\n",
        "        batch_texts = self.__get_batch_texts__(idx)\n",
        "        batch_y = self.__get_batch_labels__(idx)\n",
        "\n",
        "        return batch_texts, batch_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-U3j1mcA1tuc",
      "metadata": {
        "id": "-U3j1mcA1tuc"
      },
      "outputs": [],
      "source": [
        "#custom bert model for pytorch from huggingface\n",
        "class BertClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, dropout=0.5):\n",
        "\n",
        "        #huggingface bert\n",
        "        super(BertClassifier, self).__init__()\n",
        "\n",
        "        #define bert + dropout layer + linear layer\n",
        "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(768, 5)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "\n",
        "        #huggingface bert needs to be given the input_ids (aka tokens) and the attention_masks from the huggingface bert tokenizer\n",
        "        _, pooled_output = self.bert(input_ids= input_ids, attention_mask=attention_mask,return_dict=False)\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        linear_output = self.linear(dropout_output)\n",
        "        #don't forget to use softmax after you made a forward pass!\n",
        "\n",
        "        return linear_output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4987a453-0ef2-4d04-9935-a76d5cafb978",
      "metadata": {
        "id": "4987a453-0ef2-4d04-9935-a76d5cafb978"
      },
      "source": [
        "### Train Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2eb07c4a-4c3c-4576-a61a-dadf37edeab8",
      "metadata": {
        "id": "2eb07c4a-4c3c-4576-a61a-dadf37edeab8"
      },
      "outputs": [],
      "source": [
        "def train(model, train_data, val_data, learning_rate, epochs, noise_rate, noise_type='clean'):\n",
        "\n",
        "    #load data    \n",
        "\n",
        "    train = BertDataset(train_data, noise_type= noise_type, noise_rate=noise_rate)\n",
        "    val = BertDataset(val_data, noise_type= noise_type, noise_rate=noise_rate)\n",
        "\n",
        "    train_dataloader = DataLoader(train, batch_size=8, shuffle=True)\n",
        "    val_dataloader = DataLoader(val, batch_size=8)\n",
        "\n",
        "    #set hyperparams\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = Adam(model.parameters(), lr= learning_rate, weight_decay=1e-1)\n",
        "\n",
        "    #use gpu\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    if use_cuda:\n",
        "            model = model.cuda()\n",
        "            criterion = criterion.cuda()\n",
        "\n",
        "    #START TRAIN LOOP\n",
        "    for epoch_num in range(epochs):\n",
        "\n",
        "            #### Training ####\n",
        "\n",
        "            #init params\n",
        "            total_acc_train = 0\n",
        "            total_loss_train = 0\n",
        "\n",
        "            predicted_labels_train = []\n",
        "            real_labels_train = []\n",
        "\n",
        "            #batch loop\n",
        "            for train_input, train_label in tqdm(train_dataloader):\n",
        "\n",
        "                #move data to cuda\n",
        "                train_label = train_label.to(device)\n",
        "                mask = train_input['attention_mask'].to(device)\n",
        "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                #let model predict\n",
        "                output = model(input_id, mask)\n",
        "                \n",
        "                #compute train loss of batch\n",
        "                batch_loss = criterion(output, train_label)\n",
        "\n",
        "                #add train loss to total loss\n",
        "                total_loss_train += batch_loss.item()\n",
        "                \n",
        "\n",
        "                #add softmax\n",
        "                prob = F.softmax(output, dim=1)\n",
        "                predicted_label = prob.argmax(dim=1)\n",
        "\n",
        "                #accuracy\n",
        "                acc = (predicted_label == train_label).sum().item()\n",
        "                total_acc_train += acc\n",
        "\n",
        "                #other metrics\n",
        "                predicted_labels_train.extend(predicted_label.cpu())\n",
        "                real_labels_train.extend(train_label.cpu())\n",
        "\n",
        "                #backprop, optimizer step\n",
        "                model.zero_grad()\n",
        "                batch_loss.backward()\n",
        "                optimizer.step()\n",
        "            \n",
        "            #### Validation ####\n",
        "            #same as above but without backprop etc\n",
        "\n",
        "            total_acc_val = 0\n",
        "            total_loss_val = 0\n",
        "\n",
        "            predicted_labels_val = []\n",
        "            real_labels_val = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "\n",
        "                for val_input, val_label in val_dataloader:\n",
        "\n",
        "                    #to cuda\n",
        "                    val_label = val_label.to(device)\n",
        "                    mask = val_input['attention_mask'].to(device)\n",
        "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                    #predict\n",
        "                    output = model(input_id, mask)\n",
        "\n",
        "                    #calc loss\n",
        "                    batch_loss = criterion(output, val_label)\n",
        "                    total_loss_val += batch_loss.item()\n",
        "                    \n",
        "                    #calc accuracy\n",
        "                    prob = F.softmax(output, dim=1)\n",
        "                    predicted_label = prob.argmax(dim=1)\n",
        "\n",
        "                    acc = (predicted_label == val_label).sum().item()\n",
        "                    total_acc_val += acc\n",
        "\n",
        "                    predicted_labels_val.extend(predicted_label.cpu())\n",
        "                    real_labels_val.extend(val_label.cpu())\n",
        "            \n",
        "\n",
        "            #### Metrics ####\n",
        "\n",
        "            #after training in one epoch is done, compute the losses and accuracy measures for the epoch\n",
        "            #train metrics\n",
        "            train_loss = total_loss_train / len(train_data)\n",
        "            train_acc = total_acc_train / len(train_data)\n",
        "\n",
        "            #additional metrics\n",
        "            train_prec, train_rec, train_f, _ = prfs(real_labels_train, predicted_labels_train, average='weighted')\n",
        "\n",
        "            #val metrics\n",
        "            val_loss = total_loss_val / len(val_data)\n",
        "            val_acc = total_acc_val / len(val_data)\n",
        "\n",
        "            #additional metrics\n",
        "            val_prec, val_rec, val_f, _ = prfs(real_labels_val, predicted_labels_val, average='weighted')\n",
        "\n",
        "\n",
        "            print(\n",
        "                f'Epochs: {epoch_num + 1} | Train Loss: {train_loss: .3f} \\\n",
        "                | Train Accuracy: {train_acc: .3f} \\\n",
        "                | Val Loss: {val_loss: .3f} \\\n",
        "                | Val Accuracy: {val_acc: .3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rNy_RIPJDu7D",
      "metadata": {
        "id": "rNy_RIPJDu7D"
      },
      "outputs": [],
      "source": [
        "#evaluate on test data\n",
        "def evaluate(model, test_data, noise_type, noise_rate):\n",
        "\n",
        "    #init data\n",
        "    test = BertDataset(test_data, noise_type=noise_type, noise_rate=noise_rate)\n",
        "\n",
        "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=8)\n",
        "\n",
        "    #use gpu\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    if use_cuda:\n",
        "        model = model.cuda()\n",
        "\n",
        "    #set params\n",
        "    total_acc_test = 0\n",
        "\n",
        "    predicted_labels_test = []\n",
        "    real_labels_test = []    \n",
        "\n",
        "    #evaluate model\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for test_input, test_label in test_dataloader:\n",
        "\n",
        "            test_label = test_label.to(device)\n",
        "            mask = test_input['attention_mask'].to(device)\n",
        "            input_id = test_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "            output = model(input_id, mask)\n",
        "            prob = F.softmax(output, dim=1)\n",
        "            predicted_label = prob.argmax(dim=1)\n",
        "\n",
        "            acc = (predicted_label == test_label).sum().item()\n",
        "            total_acc_test += acc\n",
        "\n",
        "            predicted_labels_test.extend(predicted_label.cpu())\n",
        "            real_labels_test.extend(test_label.cpu())\n",
        "    #metrics\n",
        "    test_acc = total_acc_test / len(test_data)\n",
        "    #additional metrics\n",
        "    test_prec, test_rec, test_f, _ = prfs(real_labels_test, predicted_labels_test, average='weighted')\n",
        "\n",
        "\n",
        "    print(f'Test Accuracy: {test_acc: .3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6e5aec9-1417-46e7-81f8-4d2b86f5f570",
      "metadata": {
        "id": "e6e5aec9-1417-46e7-81f8-4d2b86f5f570"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96f26b5c-bef2-4a99-a88d-20d291219e5d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96f26b5c-bef2-4a99-a88d-20d291219e5d",
        "outputId": "19b937e3-3b31-4675-e5e1-4eed27066c24"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4 5\n",
            "1780\n",
            "Actual noise 0.51\n",
            "[[0.5   0.125 0.125 0.125 0.125]\n",
            " [0.125 0.5   0.125 0.125 0.125]\n",
            " [0.125 0.125 0.5   0.125 0.125]\n",
            " [0.125 0.125 0.125 0.5   0.125]\n",
            " [0.125 0.125 0.125 0.125 0.5  ]]\n",
            "4 5\n",
            "222\n",
            "Actual noise 0.49\n",
            "[[0.5   0.125 0.125 0.125 0.125]\n",
            " [0.125 0.5   0.125 0.125 0.125]\n",
            " [0.125 0.125 0.5   0.125 0.125]\n",
            " [0.125 0.125 0.125 0.5   0.125]\n",
            " [0.125 0.125 0.125 0.125 0.5  ]]\n",
            "True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 223/223 [01:37<00:00,  2.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 1 | Train Loss:  0.213                 | Train Accuracy:  0.220                 | Val Loss:  0.212                 | Val Accuracy:  0.239\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 223/223 [01:37<00:00,  2.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 2 | Train Loss:  0.210                 | Train Accuracy:  0.220                 | Val Loss:  0.205                 | Val Accuracy:  0.261\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 223/223 [01:37<00:00,  2.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 3 | Train Loss:  0.208                 | Train Accuracy:  0.220                 | Val Loss:  0.209                 | Val Accuracy:  0.185\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 223/223 [01:37<00:00,  2.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 4 | Train Loss:  0.206                 | Train Accuracy:  0.225                 | Val Loss:  0.204                 | Val Accuracy:  0.239\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 223/223 [01:37<00:00,  2.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 5 | Train Loss:  0.205                 | Train Accuracy:  0.225                 | Val Loss:  0.206                 | Val Accuracy:  0.225\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 223/223 [01:37<00:00,  2.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 6 | Train Loss:  0.202                 | Train Accuracy:  0.244                 | Val Loss:  0.204                 | Val Accuracy:  0.239\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 223/223 [01:37<00:00,  2.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 7 | Train Loss:  0.200                 | Train Accuracy:  0.274                 | Val Loss:  0.203                 | Val Accuracy:  0.284\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 223/223 [01:37<00:00,  2.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 8 | Train Loss:  0.199                 | Train Accuracy:  0.272                 | Val Loss:  0.198                 | Val Accuracy:  0.279\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 223/223 [01:37<00:00,  2.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 9 | Train Loss:  0.196                 | Train Accuracy:  0.306                 | Val Loss:  0.199                 | Val Accuracy:  0.279\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 223/223 [01:37<00:00,  2.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 10 | Train Loss:  0.197                 | Train Accuracy:  0.292                 | Val Loss:  0.195                 | Val Accuracy:  0.293\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 223/223 [01:37<00:00,  2.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 11 | Train Loss:  0.195                 | Train Accuracy:  0.321                 | Val Loss:  0.200                 | Val Accuracy:  0.324\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 223/223 [01:37<00:00,  2.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 12 | Train Loss:  0.194                 | Train Accuracy:  0.320                 | Val Loss:  0.197                 | Val Accuracy:  0.297\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 223/223 [01:37<00:00,  2.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 13 | Train Loss:  0.191                 | Train Accuracy:  0.330                 | Val Loss:  0.194                 | Val Accuracy:  0.338\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 223/223 [01:37<00:00,  2.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 14 | Train Loss:  0.191                 | Train Accuracy:  0.335                 | Val Loss:  0.200                 | Val Accuracy:  0.275\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 223/223 [01:37<00:00,  2.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 15 | Train Loss:  0.188                 | Train Accuracy:  0.356                 | Val Loss:  0.194                 | Val Accuracy:  0.333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 223/223 [01:37<00:00,  2.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 16 | Train Loss:  0.188                 | Train Accuracy:  0.366                 | Val Loss:  0.196                 | Val Accuracy:  0.329\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 223/223 [01:37<00:00,  2.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 17 | Train Loss:  0.187                 | Train Accuracy:  0.376                 | Val Loss:  0.190                 | Val Accuracy:  0.378\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 223/223 [01:37<00:00,  2.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 18 | Train Loss:  0.185                 | Train Accuracy:  0.385                 | Val Loss:  0.189                 | Val Accuracy:  0.369\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 223/223 [01:37<00:00,  2.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 19 | Train Loss:  0.183                 | Train Accuracy:  0.400                 | Val Loss:  0.188                 | Val Accuracy:  0.392\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 223/223 [01:37<00:00,  2.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 20 | Train Loss:  0.182                 | Train Accuracy:  0.418                 | Val Loss:  0.189                 | Val Accuracy:  0.392\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 223/223 [01:37<00:00,  2.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 21 | Train Loss:  0.180                 | Train Accuracy:  0.417                 | Val Loss:  0.191                 | Val Accuracy:  0.410\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 223/223 [01:37<00:00,  2.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 22 | Train Loss:  0.179                 | Train Accuracy:  0.442                 | Val Loss:  0.188                 | Val Accuracy:  0.432\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 223/223 [01:37<00:00,  2.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 23 | Train Loss:  0.176                 | Train Accuracy:  0.469                 | Val Loss:  0.189                 | Val Accuracy:  0.387\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 223/223 [01:37<00:00,  2.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 24 | Train Loss:  0.175                 | Train Accuracy:  0.452                 | Val Loss:  0.190                 | Val Accuracy:  0.378\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 223/223 [01:37<00:00,  2.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 25 | Train Loss:  0.172                 | Train Accuracy:  0.469                 | Val Loss:  0.190                 | Val Accuracy:  0.441\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 25\n",
        "LR = 1e-6\n",
        "NOISE_RATES = [0.2,0.4,0.5]\n",
        "NOISE_TYPE = 'symmetric'\n",
        "model = BertClassifier()\n",
        "\n",
        "\n",
        "#start train loop for clean dataset\n",
        "train(model, df_train, df_val, LR, EPOCHS, 0.0, 'clean')\n",
        "#evaluate model on clean test data\n",
        "evaluate(model, df_test, 'clean', 0.0)\n",
        "\n",
        "\n",
        "#start train loop for different noise rates:\n",
        "for NOISE_RATE in NOISE_RATES:\n",
        "    print('#########################################################################')\n",
        "    print('NOISE RATE:',NOISE_RATE)\n",
        "    train(model, df_train, df_val, LR, EPOCHS, NOISE_RATE, NOISE_TYPE)\n",
        "    #evaluate model on clean test data\n",
        "    evaluate(model, df_test, 'clean', 0.0)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "name": "BERT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "43f84272b90556fb4a83508143e57e998eabb218d530dd495b531740d9060792"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
